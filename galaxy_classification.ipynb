{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy Classification Neural Network\n",
    "\n",
    "This notebook trains a convolutional neural network to classify galaxies using the Galaxy10 DECals dataset. It explores different optimizers and learning rates to find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 21:38:27.895310: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-12 21:38:27.937792: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 21:38:29.234892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mateo/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from importData import load_galaxy_data\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The following function preprocesses the dataset by resizing images to 64x64 pixels and normalizing them. The dataset is then split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    new_size = (64, 64)\n",
    "    images = np.array([np.array(Image.fromarray(np.array(image['image'])).resize(new_size)) for image in dataset['train']])\n",
    "    labels = np.array(dataset['train']['label'])\n",
    "\n",
    "    images = images / 255.0\n",
    "\n",
    "    return train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "The following function builds a convolutional neural network model with specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layer1_neuron, layer2_neuron, layer3_neuron, layer4_neuron, lr, kernel_size, activation_function, dropout_value, opt_name):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=(64, 64, 3)),\n",
    "        tf.keras.layers.Conv2D(layer1_neuron, kernel_size, activation=activation_function),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(layer2_neuron, kernel_size, activation=activation_function),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(layer3_neuron, kernel_size, activation=activation_function),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(layer4_neuron, activation=activation_function),\n",
    "        tf.keras.layers.Dropout(dropout_value),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=opt_name(learning_rate=lr),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here we define the parameters for the neural network, optimizers, and learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m activation_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m dropout_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     12\u001b[0m optimizers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdagrad\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdagrad,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m learning_rates \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;241m1e-2\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "nb_epochs = 15  # Number of epochs\n",
    "layer1_neuron = 32\n",
    "layer2_neuron = 64\n",
    "layer3_neuron = 128\n",
    "layer4_neuron = 64\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "activation_function = 'relu'\n",
    "dropout_value = 0.5\n",
    "\n",
    "optimizers = {\n",
    "    'SGD': tf.keras.optimizers.SGD,\n",
    "    'Adagrad': tf.keras.optimizers.Adagrad,\n",
    "    'Adam': tf.keras.optimizers.Adam\n",
    "    # Add others optimizers to test\n",
    "}\n",
    "\n",
    "learning_rates = [1e-3, 1e-2, 1e-1] # Add others learning rates to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the Galaxy10 DECals dataset and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading Dataset\\n\")\n",
    "dataset = load_galaxy_data()\n",
    "train_images, test_images, train_labels, test_labels = preprocess_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Iterate through each optimizer and learning rate combination, train the model, and evaluate its performance. TensorBoard is used for monitoring the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterate on each optimizer and each learning_rate\n",
      "\n",
      "\n",
      "Created runs/Adam_lr_0.001_20240612-214021 file\n",
      "\n",
      "\n",
      "Training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 21:40:23.155274: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-12 21:40:23.155849: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 39ms/step - accuracy: 0.1773 - loss: 2.1669 - val_accuracy: 0.2753 - val_loss: 1.8373\n",
      "Epoch 2/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.2764 - loss: 1.8948 - val_accuracy: 0.4015 - val_loss: 1.6370\n",
      "Epoch 3/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.3509 - loss: 1.7040 - val_accuracy: 0.4444 - val_loss: 1.4754\n",
      "Epoch 4/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.4012 - loss: 1.5561 - val_accuracy: 0.4795 - val_loss: 1.3557\n",
      "Epoch 5/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.4423 - loss: 1.4577 - val_accuracy: 0.5315 - val_loss: 1.2636\n",
      "Epoch 6/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.4905 - loss: 1.3602 - val_accuracy: 0.5427 - val_loss: 1.2183\n",
      "Epoch 7/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.5126 - loss: 1.3330 - val_accuracy: 0.5772 - val_loss: 1.1925\n",
      "Epoch 8/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.5501 - loss: 1.2504 - val_accuracy: 0.5672 - val_loss: 1.2131\n",
      "Epoch 9/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.5205 - loss: 1.3037 - val_accuracy: 0.6261 - val_loss: 1.0940\n",
      "Epoch 10/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.5733 - loss: 1.1487 - val_accuracy: 0.6267 - val_loss: 1.0771\n",
      "Epoch 11/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.5971 - loss: 1.1163 - val_accuracy: 0.6286 - val_loss: 1.0713\n",
      "Epoch 12/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.6026 - loss: 1.0822 - val_accuracy: 0.6239 - val_loss: 1.0905\n",
      "Epoch 13/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.6174 - loss: 1.0406 - val_accuracy: 0.6530 - val_loss: 1.0544\n",
      "Epoch 14/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.6263 - loss: 1.0021 - val_accuracy: 0.6182 - val_loss: 1.0756\n",
      "Epoch 15/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.6413 - loss: 0.9443 - val_accuracy: 0.6621 - val_loss: 1.0327\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6712 - loss: 1.0273\n",
      "Optimizer: Adam, lr: 0.001 - Test Accuracy: 66.21%\n",
      "\n",
      "\n",
      "Best combination: (Optimizer: Adam - Learning Rate: 0.001 - Accuracy: 66.2073)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_opt = ''\n",
    "best_lr = 0\n",
    "best_acc = 0\n",
    "\n",
    "print(\"\\nIterate on each optimizer and each learning_rate\\n\")\n",
    "for opt_name, opt_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        # Create a logs directory\n",
    "        log_dir = f\"runs/{opt_name}_lr_{lr}_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(f\"\\nCreated {log_dir} file\\n\")\n",
    "\n",
    "        # Training and Testing neural network\n",
    "        print(\"\\nTraining...\\n\")\n",
    "        model = build_model(layer1_neuron, layer2_neuron, layer3_neuron, layer4_neuron, lr, kernel_size, activation_function, dropout_value, opt_class)\n",
    "        model.fit(train_images, train_labels, epochs=nb_epochs, validation_data=(test_images, test_labels), callbacks=[tensorboard_callback])\n",
    "        test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "        if best_acc < test_acc:\n",
    "            best_acc = test_acc\n",
    "            best_lr = lr\n",
    "            best_opt = opt_name\n",
    "        print(f\"Optimizer: {opt_name}, lr: {lr} - Test Accuracy: {test_acc*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"\\nBest combination: (Optimizer: {best_opt} - Learning Rate: {best_lr} - Accuracy: {best_acc*100:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running TensorBoard\n",
    "\n",
    "To monitor the training process, run the following command in your terminal:\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "Then open a web browser and go to the URL provided by TensorBoard (typically `http://localhost:6006`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
