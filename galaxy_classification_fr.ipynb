{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de Neurones pour la Classification des Galaxies\n",
    "\n",
    "Ce carnet entraîne un réseau de neurones convolutionnel pour classifier les galaxies en utilisant le dataset Galaxy10 DECals. Il explore différents optimisateurs et taux d'apprentissage pour trouver la meilleure combinaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 23:14:29.978813: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-12 23:14:30.021287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 23:14:30.790545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mateo/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement des Données\n",
    "\n",
    "La fonction suivante prétraite le dataset en redimensionnant les images à 64x64 pixels et en les normalisant. Le dataset est ensuite divisé en ensembles d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    new_size = (64, 64)\n",
    "    images = np.array([np.array(Image.fromarray(np.array(image['image'])).resize(new_size)) for image in dataset['train']])\n",
    "    labels = np.array(dataset['train']['label'])\n",
    "\n",
    "    images = images / 255.0\n",
    "\n",
    "    return train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du Modèle\n",
    "\n",
    "La fonction suivante construit un modèle de réseau de neurones convolutionnel avec les paramètres spécifiés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layer1_neuron, layer2_neuron, layer3_neuron, layer4_neuron, lr, kernel_size, activation_function, dropout_value, opt_name):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=(64, 64, 3)),\n",
    "        tf.keras.layers.Conv2D(layer1_neuron, kernel_size, activation=activation_function),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(layer2_neuron, kernel_size, activation=activation_function),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(layer3_neuron, kernel_size, activation=activation_function),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(layer4_neuron, activation=activation_function),\n",
    "        tf.keras.layers.Dropout(dropout_value),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=opt_name(learning_rate=lr),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres\n",
    "\n",
    "Nous définissons ici les paramètres pour le réseau de neurones, les optimiseurs, et les taux d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "nb_epochs = 15  # Nombre d'époques\n",
    "layer1_neuron = 32\n",
    "layer2_neuron = 64\n",
    "layer3_neuron = 128\n",
    "layer4_neuron = 64\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "activation_function = 'relu'\n",
    "dropout_value = 0.5\n",
    "\n",
    "optimizers = {\n",
    "    'SGD': tf.keras.optimizers.SGD,\n",
    "    'Adagrad': tf.keras.optimizers.Adagrad,\n",
    "    'Adam': tf.keras.optimizers.Adam\n",
    "    # Rajouter d'autres optimizers pour tester\n",
    "}\n",
    "\n",
    "learning_rates = [1e-3, 1e-2, 1e-1]# Rajouter d'autres learning rates pour tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du Dataset\n",
    "\n",
    "Charger le dataset Galaxy10 DECals et le prétraiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chargement du Dataset...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChargement du Dataset...\\n\")\n",
    "dataset = load_dataset(\"matthieulel/galaxy10_decals\")\n",
    "train_images, test_images, train_labels, test_labels = preprocess_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement et Évaluation\n",
    "\n",
    "Itérer à travers chaque combinaison d'optimiseur et de taux d'apprentissage, entraîner le modèle et évaluer ses performances. TensorBoard est utilisé pour surveiller le processus d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Itération sur chaque optimiseur et chaque taux d'apprentissage\n",
      "\n",
      "\n",
      "Création du fichier runs/Adam_lr_0.001_20240612-231622\n",
      "\n",
      "\n",
      "Entraînement...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 23:16:29.875977: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-12 23:16:29.882710: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 42ms/step - accuracy: 0.2138 - loss: 2.1050 - val_accuracy: 0.4366 - val_loss: 1.6072\n",
      "Epoch 2/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.3862 - loss: 1.6714 - val_accuracy: 0.5343 - val_loss: 1.3628\n",
      "Epoch 3/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.4771 - loss: 1.4334 - val_accuracy: 0.5393 - val_loss: 1.2693\n",
      "Epoch 4/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.5372 - loss: 1.3041 - val_accuracy: 0.5800 - val_loss: 1.1735\n",
      "Epoch 5/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 41ms/step - accuracy: 0.5680 - loss: 1.2256 - val_accuracy: 0.6342 - val_loss: 1.0846\n",
      "Epoch 6/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.5934 - loss: 1.1453 - val_accuracy: 0.6311 - val_loss: 1.0457\n",
      "Epoch 7/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6259 - loss: 1.0677 - val_accuracy: 0.6514 - val_loss: 1.0222\n",
      "Epoch 8/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6473 - loss: 0.9959 - val_accuracy: 0.6217 - val_loss: 1.0885\n",
      "Epoch 9/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6663 - loss: 0.9659 - val_accuracy: 0.6477 - val_loss: 1.0050\n",
      "Epoch 10/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6875 - loss: 0.9005 - val_accuracy: 0.6687 - val_loss: 0.9588\n",
      "Epoch 11/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.6940 - loss: 0.8474 - val_accuracy: 0.6524 - val_loss: 1.0529\n",
      "Epoch 12/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.7050 - loss: 0.8247 - val_accuracy: 0.6790 - val_loss: 0.9578\n",
      "Epoch 13/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7342 - loss: 0.7568 - val_accuracy: 0.6502 - val_loss: 1.0575\n",
      "Epoch 14/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.7263 - loss: 0.7624 - val_accuracy: 0.6896 - val_loss: 0.9313\n",
      "Epoch 15/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.7340 - loss: 0.7363 - val_accuracy: 0.6806 - val_loss: 1.0114\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6888 - loss: 0.9859\n",
      "Optimiseur: Adam, lr: 0.001 - Précision sur le test: 68.06%\n",
      "\n",
      "\n",
      "Meilleure combinaison: (Optimiseur: Adam - Taux d'Apprentissage: 0.001 - Précision: 68.0551)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_opt = ''\n",
    "best_lr = 0\n",
    "best_acc = 0\n",
    "\n",
    "print(\"\\nItération sur chaque optimiseur et chaque taux d'apprentissage\\n\")\n",
    "for opt_name, opt_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        # Créer un répertoire de logs\n",
    "        log_dir = f\"runs/{opt_name}_lr_{lr}_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(f\"\\nCréation du fichier {log_dir}\\n\")\n",
    "\n",
    "        # Entraînement et test du réseau de neurones\n",
    "        print(\"\\nEntraînement...\\n\")\n",
    "        model = build_model(layer1_neuron, layer2_neuron, layer3_neuron, layer4_neuron, lr, kernel_size, activation_function, dropout_value, opt_class)\n",
    "        model.fit(train_images, train_labels, epochs=nb_epochs, validation_data=(test_images, test_labels), callbacks=[tensorboard_callback])\n",
    "        test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "        if best_acc < test_acc:\n",
    "            best_acc = test_acc\n",
    "            best_lr = lr\n",
    "            best_opt = opt_name\n",
    "        print(f\"Optimiseur: {opt_name}, lr: {lr} - Précision sur le test: {test_acc*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"\\nMeilleure combinaison: (Optimiseur: {best_opt} - Taux d'Apprentissage: {best_lr} - Précision: {best_acc*100:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exécution de TensorBoard\n",
    "\n",
    "Pour surveiller le processus d'entraînement, exécutez la commande suivante dans votre terminal :\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "Ensuite, ouvrez un navigateur web et allez à l'URL fournie par TensorBoard (généralement `http://localhost:6006`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
