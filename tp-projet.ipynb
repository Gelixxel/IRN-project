{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de Neurones pour la Classification des Galaxies\n",
    "\n",
    "Ce carnet entraîne un réseau de neurones convolutionnel pour classifier les galaxies en utilisant le dataset Galaxy10 DECals. Il explore différents optimisateurs et taux d'apprentissage pour trouver la meilleure combinaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement des Données\n",
    "\n",
    "La fonction suivante prétraite le dataset en redimensionnant les images à 64x64 pixels et en les normalisant. Le dataset est ensuite divisé en ensembles d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    new_size = (64, 64)\n",
    "    images = np.array([np.array(Image.fromarray(np.array(image['image'])).resize(new_size)) for image in dataset['train']])\n",
    "    labels = np.array(dataset['train']['label'])\n",
    "\n",
    "    images = images / 255.0\n",
    "\n",
    "    return train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du Modèle\n",
    "\n",
    "La fonction suivante construit un modèle de réseau de neurones convolutionnel avec les paramètres spécifiés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layer1_neuron, layer2_neuron, layer3_neuron, layer4_neuron, lr, kernel_size, activation_function, dropout_value, opt_name):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=(64, 64, 3)),\n",
    "        tf.keras.layers.Conv2D(layer1_neuron, kernel_size, activation=activation_function),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(layer2_neuron, kernel_size, activation=activation_function),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(layer3_neuron, kernel_size, activation=activation_function),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(layer4_neuron, activation=activation_function),\n",
    "        tf.keras.layers.Dropout(dropout_value),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=opt_name(learning_rate=lr),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres\n",
    "\n",
    "Nous définissons ici les paramètres pour le réseau de neurones, les optimiseurs, et les taux d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "nb_epochs = 15  # Nombre d'époques\n",
    "layer1_neuron = 32\n",
    "layer2_neuron = 64\n",
    "layer3_neuron = 128\n",
    "layer4_neuron = 64\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "activation_function = 'relu'\n",
    "dropout_value = 0.5\n",
    "\n",
    "optimizers = {\n",
    "    'SGD': tf.keras.optimizers.SGD,\n",
    "    'Adagrad': tf.keras.optimizers.Adagrad,\n",
    "    'Adam': tf.keras.optimizers.Adam\n",
    "    # Rajouter d'autres optimizers pour tester\n",
    "}\n",
    "\n",
    "learning_rates = [1e-3, 1e-2, 1e-1]# Rajouter d'autres learning rates pour tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du Dataset\n",
    "\n",
    "Charger le dataset Galaxy10 DECals et le prétraiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chargement du Dataset...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChargement du Dataset...\\n\")\n",
    "dataset = load_dataset(\"matthieulel/galaxy10_decals\")\n",
    "train_images, test_images, train_labels, test_labels = preprocess_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement et Évaluation\n",
    "\n",
    "Itérer à travers chaque combinaison d'optimiseur et de taux d'apprentissage, entraîner le modèle et évaluer ses performances. TensorBoard est utilisé pour surveiller le processus d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Itération sur chaque optimiseur et chaque taux d'apprentissage\n",
      "\n",
      "\n",
      "Création du fichier runs/Adam_lr_0.001_20240612-215407\n",
      "\n",
      "\n",
      "Entraînement...\n",
      "\n",
      "Epoch 1/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.1740 - loss: 2.1807 - val_accuracy: 0.3667 - val_loss: 1.7297\n",
      "Epoch 2/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.3403 - loss: 1.7436 - val_accuracy: 0.4839 - val_loss: 1.4524\n",
      "Epoch 3/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.4040 - loss: 1.5663 - val_accuracy: 0.5290 - val_loss: 1.3700\n",
      "Epoch 4/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.4538 - loss: 1.4829 - val_accuracy: 0.5152 - val_loss: 1.2899\n",
      "Epoch 5/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.4821 - loss: 1.3732 - val_accuracy: 0.5171 - val_loss: 1.2818\n",
      "Epoch 6/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.5119 - loss: 1.3067 - val_accuracy: 0.5985 - val_loss: 1.1627\n",
      "Epoch 7/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.5300 - loss: 1.2825 - val_accuracy: 0.6135 - val_loss: 1.0983\n",
      "Epoch 8/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.5486 - loss: 1.2224 - val_accuracy: 0.6210 - val_loss: 1.0917\n",
      "Epoch 9/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.5720 - loss: 1.1687 - val_accuracy: 0.6376 - val_loss: 1.0945\n",
      "Epoch 10/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.6032 - loss: 1.1050 - val_accuracy: 0.6408 - val_loss: 1.0478\n",
      "Epoch 11/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.6297 - loss: 1.0463 - val_accuracy: 0.6486 - val_loss: 1.0371\n",
      "Epoch 12/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.6392 - loss: 0.9971 - val_accuracy: 0.6345 - val_loss: 1.0655\n",
      "Epoch 13/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.6287 - loss: 1.0452 - val_accuracy: 0.6580 - val_loss: 1.0777\n",
      "Epoch 14/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.6281 - loss: 1.0581 - val_accuracy: 0.6821 - val_loss: 0.9792\n",
      "Epoch 15/15\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.6809 - loss: 0.8853 - val_accuracy: 0.6693 - val_loss: 1.0016\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6838 - loss: 0.9612\n",
      "Optimiseur: Adam, lr: 0.001 - Précision sur le test: 66.93%\n",
      "\n",
      "\n",
      "Meilleure combinaison: (Optimiseur: Adam - Taux d'Apprentissage: 0.001 - Précision: 66.9277)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_opt = ''\n",
    "best_lr = 0\n",
    "best_acc = 0\n",
    "\n",
    "print(\"\\nItération sur chaque optimiseur et chaque taux d'apprentissage\\n\")\n",
    "for opt_name, opt_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        # Créer un répertoire de logs\n",
    "        log_dir = f\"runs/{opt_name}_lr_{lr}_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(f\"\\nCréation du fichier {log_dir}\\n\")\n",
    "\n",
    "        # Entraînement et test du réseau de neurones\n",
    "        print(\"\\nEntraînement...\\n\")\n",
    "        model = build_model(layer1_neuron, layer2_neuron, layer3_neuron, layer4_neuron, lr, kernel_size, activation_function, dropout_value, opt_class)\n",
    "        model.fit(train_images, train_labels, epochs=nb_epochs, validation_data=(test_images, test_labels), callbacks=[tensorboard_callback])\n",
    "        test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "        if best_acc < test_acc:\n",
    "            best_acc = test_acc\n",
    "            best_lr = lr\n",
    "            best_opt = opt_name\n",
    "        print(f\"Optimiseur: {opt_name}, lr: {lr} - Précision sur le test: {test_acc*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"\\nMeilleure combinaison: (Optimiseur: {best_opt} - Taux d'Apprentissage: {best_lr} - Précision: {best_acc*100:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exécution de TensorBoard\n",
    "\n",
    "Pour surveiller le processus d'entraînement, exécutez la commande suivante dans votre terminal :\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "Ensuite, ouvrez un navigateur web et allez à l'URL fournie par TensorBoard (généralement `http://localhost:6006`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
